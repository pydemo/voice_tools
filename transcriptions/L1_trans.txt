This lesson introduces the concept of the *Conversable Agent*, a built-in agent class of AutoGen that can be used to construct multi-agent conversations. You will learn about the basic functionality and build your first two-agent chat that showcases a fun conversation between two stand-up comedians. Let's dive in.

Let's begin by understanding the concept of an agent. In AutoGen, an agent is an entity that can act on behalf of human intent, send messages, receive messages, perform actions, generate replies, and interact with other agents. AutoGen includes a built-in agent class called *Conversable Agent*, which unifies different types of agents within the same programming abstraction. It offers many built-in functionalities, such as using a list of LLM configurations to generate replies, performing code execution, and executing functions and tools. It also provides a component for keeping humans in the loop and checking for stopping responses. You can switch each component on and off and customize it to suit your application's needs. Using these capabilities, you can create agents with different roles using the same interface.

To start, let's import the OpenAPI key from the environment using the `getOpenAPIKey` utility function. Run it to get the OpenAPI key and then define an LLM configuration. In this course, we will use GPT-3.5 Turbo as the model. Next, let's import the *Conversable Agent* class from AutoGen and create our first *Conversable Agent* object. We define an agent named "Chatbot" using the *Conversable Agent* class and pass the LLM configuration we defined earlier. This agent will use the large language model to generate replies. We also set the human input mode to "never," meaning the agent will not seek human input and will only use the language model to generate responses.

Generally, you could switch the human input mode to other settings. For example, if you set it to "always," the agent will always ask for human input before trying to generate a reply on its own. These are only the basic setups for this agent. You could also add configurations for code execution, function execution, and other settings, but let's start with this simple setup.

The first thing you can do is ask this agent to generate a response to a question using the `generateReply` method. We call this agent's `generateReply` function and provide it with a message list containing the content, "Tell me a joke," and the role set as "user." When we run this, the agent replies, "Sure, here's a joke for you. Why did the scarecrow win the award? Because he was outstanding in his field." This is the most basic interaction, where you ask a question and receive a response from the agent.

Now, if you call this function again with a new message, such as "Repeat the joke," do you expect the agent to repeat the same joke? Actually, no. When you call the `generateReply` function, it doesn’t alter the internal state of the agent. So, when the function is called again, it doesn’t know that it has generated a reply before and will generate a fresh response without remembering the previous interaction. You could use this in applications to generate different replies if needed. However, if you want to maintain the state and perform a series of tasks, you need a different approach.

In the next part, let's look at how to create a conversation between multiple agents using a stand-up comedy example. We want to create an application where two stand-up comedians talk to each other and make fun of each other. First, we create a *Conversable Agent* named "Cassie." We give it a system message stating, "Your name is Cassie, and you're a stand-up comedian," and pass the same LLM configuration and human input mode. If you don’t specify this system message, the agent will have an empty system message and perform as a general-purpose assistant agent. This message customizes the agent's behavior.

Next, let's add another agent. We create a *Conversable Agent* named "Joe" and give it the system message, "Your name is Joe, and you're a stand-up comedian." We also add an instruction, "Start the next joke from the punchline of the previous joke," providing specific guidance on how to carry over the conversation.

With both comedians created, we can initiate the conversation. We call the `initiateChat` function from one of the agents, setting "Joe" as the sender, "Cassie" as the recipient, and giving the initial message, "I'm Joe, Cassie, let's keep the jokes rolling." We set the max turns to 2, so they will exchange two turns of conversation and then finish. Here’s what happens:

Joe starts with, "I'm Joe, Cassie, let's keep the jokes rolling." Cassie responds, "Hey Joe, great to meet another comedian! Let’s start with some jokes. Why did the math book look sad? Because it had too many problems." Joe follows up with, "Well, Cassie, at least now we know why the math book was always so negative." Cassie concludes with, "Exactly, it just couldn't subtract the sadness from its pages." The conversation stops after two turns.

After the conversation ends, we can inspect the chat history using the `pprint` library. We see the messages exchanged: first from Joe, then Cassie, Joe again, and finally Cassie. We can also check the token usage in the chat result. Calling the `chatResult.cost` function shows we used 97 completion tokens, 219 prompt tokens, totaling 316 tokens, and the associated cost. You can define conversations in different ways and check the summary of the chat result using the `chatResult.summary` function. By default, it uses the last message as the summary.

If you want to change the summary method, you can configure it with a different method, such as "reflection with LLM," and provide a summary prompt like "Summarize the conversation." After the conversation finishes, the language model reflects on the conversation and produces a new summary. For instance, after using a different summary method, the summary might be, "The conversation focused on sharing jokes and puns between Joe and Cassie. They playfully exchanged math and scarecrow-related jokes to keep the laughs flowing."

You’ll notice that we used `maxTurns = 2` to control the number of turns in this conversation. What if you don’t know the right number of turns before the conversation ends? You can change the termination condition by providing an additional configuration called `isTerminationMessage`, a boolean function that takes a message as input and returns true or false, indicating whether the message signals the conversation should end. For example, we set the condition to end when someone says, "I’ve got to go." If an agent receives a message containing this phrase, it will stop replying.

After setting this condition, let's initiate the conversation again. This time, the agents will have more turns. Cassie makes a joke, Joe responds, Cassie asks for another joke, and Joe replies. Eventually, Joe ends with, "Glad you enjoyed it, Cassie. Puns are always a hit. Thanks for the laughs. I’ve got to go." Cassie recognizes the phrase and stops replying. This method offers a more flexible way of ending the conversation.

After the conversation ends, you might want to continue or check if the agent can preserve its state. Let’s test this by asking Cassie, “What was the last joke we talked about?” and set the recipient as Joe. Will Joe remember? Yes! Joe replies, “The last joke we talked about was the scarecrow winning an award because he was outstanding in his field.” Both agents follow the termination condition, and Cassie ends with, “I’ve got to go.”

This demonstrates how to mix agents, start and continue conversations, and preserve the state. This is a basic demonstration of how to use a *Conversable Agent* to construct a conversation between two agents. In the next lessons, we will explore various conversation patterns and agent design patterns, including tool usage, reflection, planning, and code execution.